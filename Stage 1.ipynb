{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1\n",
    "\n",
    "The goal of this stage is to indicate the index of the training set which is not an outlier. In other words, we do not want to include any outlier observation in the training dataset when we fit our true model. This is actually debatable as there are two opposing sides of this. One, we are not sure which training dataset is an outlier and which one is not. By removing more and more observation that we deemed as an outlier, it causes our model to be less robust as we are not allowing the model to learn about extreme cases. However, if we fail to remove true outlier, our model will be misguided.\n",
    "\n",
    "The other points that I would like to highlight over here is the methodology in determining outliers. Even though it seems weird that I am fitting a model, and then using the predicted values of the model to determine whether it is an outlier (if my prediction is far way off from the true value), it is commonly used in Kaggle community and I want to try it out! Furthermore, it is quite shortsighted to remove the observation based on a single covariate (i.e Log straight traj length vs true trajlength) because there might be other factors that we do not take into account.\n",
    "\n",
    "In this Stage 1 model, the input will be the training data with the new features that we have develop in stage 0. We will fit a Random Forest and Extreme Gradient Boosting algorithm to the training data to predict both log duration and log trajlength. To ensure that the training data that we predict is not contained in fitting the model that we use to predict, we will perform K-Fold Cross Validation to do this. Then, we will compare the prediction vs the true value of log duration and log trajlength from the training dataset. We will then remove all those values that have high percentage error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LassoCV\n",
    "import xgboost as xgb\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_stage0 = joblib.load( 'X_train_stage0.pkl')\n",
    "Y_train_duration = joblib.load( 'Y_train_duration.pkl')\n",
    "Y_train_trajlength = joblib.load( 'Y_train_trajlength.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_dur = joblib.load('rf_dur.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = X_train_stage0.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sX_train = sparse.csc_matrix(X_train_stage0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits = 3, shuffle = False, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pred_dur = np.zeros(n_train)\n",
    "bst_pred_dur = np.zeros(n_train)\n",
    "rf_pred_traj =np.zeros(n_train)\n",
    "bst_pred_traj = np.zeros(n_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Parameter of the XGBoost that I will use in this model is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dur_param = { 'objective' : \"reg:linear\", \n",
    "          'booster' : \"gbtree\",\n",
    "          'eta'                 :0.01, \n",
    "          'max_depth'           :12, \n",
    "          'colsample_bytree'    : 0.7,\n",
    "          'subsample' : 0.7,\n",
    "          'gamma' : 1,\n",
    "          'min_child_weight' : 5,\n",
    "          'n_thread' : 8\n",
    "        }\n",
    "\n",
    "traj_param = { 'objective' : \"reg:linear\", \n",
    "          'booster' : \"gbtree\",\n",
    "          'eta'                 :0.02, \n",
    "          'max_depth'           :20, \n",
    "          'colsample_bytree'    : 0.7,\n",
    "          'subsample' : 0.7,\n",
    "          'gamma' : 1,\n",
    "          'min_child_weight' : 5,\n",
    "          'n_thread' : 8\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, `sklearn`  package does not contain implementation of Root Mean Square Percentage Error. Thus, we will need to define this metric on our own. We will create the implementation for both `sklearn` and `xgboost` package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "def rmpse_loss_func(ground_truth, predictions):\n",
    "    err = np.sqrt(np.mean((np.true_divide\\\n",
    "                           (predictions, \n",
    "                            ground_truth) - 1.)**2))\n",
    "    return err\n",
    "\n",
    "rmpse_loss  = make_scorer(rmpse_loss_func, \n",
    "                          greater_is_better=False)\n",
    "\n",
    "def rmpse(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    err = np.sqrt(np.mean((\\\n",
    "                           np.true_divide\\\n",
    "                           (preds,\n",
    "                            labels) - 1.)**2))\n",
    "    return 'error', err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform this prediction for the training data against the true value of the duration and trajlength as follows:\n",
    "\n",
    "- Under the training data get the index of training index(remaining index of the other k-1 folds) and test index using the `kfold.split`. \n",
    "- Fit Random Forest model and XGBoost model using the training data under training index\n",
    "- Predict duration and trajlength values for training data under test index\n",
    "- Repeat for all K folds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index, test_index in kfold.split(sX_train):\n",
    "    sX_fold_train = sX_train[train_index]\n",
    "    sX_fold_test = sX_train[test_index]\n",
    "    y_fold_train_dur = Y_train_duration[train_index]\n",
    "    y_fold_train_traj = Y_train_trajlength[train_index]\n",
    "    \n",
    "    dtrain_dur = xgb.DMatrix(sX_fold_train,\n",
    "                             label = y_fold_train_dur)\n",
    "    dtrain_traj = xgb.DMatrix(sX_fold_train,\n",
    "                              label = y_fold_train_traj)\n",
    "    dtest = xgb.DMatrix(sX_fold_test)\n",
    "    \n",
    "    bst_dur = xgb.train(dur_param,\n",
    "                        dtrain_dur,\n",
    "                        evals=[(dtrain_dur, 'train')], \n",
    "                num_boost_round = 350,\n",
    "                        feval= rmpse, maximize = False)\n",
    "    bst_traj = xgb.train(traj_param,\n",
    "                         dtrain_traj, \n",
    "                         evals=[(dtrain_traj, 'train')], \n",
    "                num_boost_round = 400, \n",
    "                         feval= rmpse, maximize = False)\n",
    "    \n",
    "    rf_dur = \\\n",
    "    RandomForestRegressor(max_depth = 22,\n",
    "                          max_features = 'sqrt',\n",
    "                          n_estimators=500, \n",
    "                          verbose = 3,\n",
    "                          n_jobs = -1,\n",
    "                          criterion='mse').\\\n",
    "    fit(sX_fold_train, y_fold_train_dur)\n",
    "    \n",
    "    rf_traj = \\\n",
    "    RandomForestRegressor(max_depth = 22,\n",
    "                          max_features = 'sqrt', \n",
    "                          n_estimators=500, \n",
    "                          verbose = 3,\n",
    "                          n_jobs = -1,\n",
    "                          criterion='mse')\\\n",
    "    .fit(sX_fold_train, y_fold_train_traj)\n",
    "    \n",
    "    bst_pred_dur[test_index] = bst_dur.predict(dtest)\n",
    "    bst_pred_traj[test_index] = bst_traj.predict(dtest)\n",
    "    rf_pred_dur[test_index] = rf_dur.predict(sX_fold_test)\n",
    "    rf_pred_traj[test_index] = rf_traj.predict(sX_fold_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then find the percentage difference between the predicted duration and trajlength values from the training data using the K-Fold Gradient Boosting & Random Forest model and the true value of the duration and trajlength values.\n",
    "\n",
    "We will then assume that the value is an outlier whenever the squared percentage difference between the true value and the predicted value is more than 0.10 for any prediction value - Gradient Boosting for Duration, Random Forest for Duration, Gradient Boosting for Trajlength, and Random Forest for Trajlength\n",
    "\n",
    "Using this method, we throw out 1021 Outlier values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perc_diff(ground_truth, predictions):\n",
    "    return (np.true_divide(predictions, ground_truth) - 1.)**2\n",
    "\n",
    "bst_diff_dur = perc_diff(Y_train_duration, bst_pred_dur)\n",
    "bst_diff_traj = perc_diff(Y_train_trajlength, bst_pred_traj)\n",
    "rf_diff_dur = perc_diff(Y_train_duration, rf_pred_dur)\n",
    "rf_diff_traj = perc_diff(Y_train_trajlength, rf_pred_traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_result = {\n",
    "    'bst_diff_dur' : bst_diff_dur,\n",
    "    'bst_diff_traj' : bst_diff_traj,\n",
    "    'rf_diff_dur' : rf_diff_dur,\n",
    "    'rf_diff_traj' : rf_diff_traj\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(dict_result)\n",
    "df['outlier_bst_diff_dur'] = np.where(df.bst_diff_dur.values > 0.1, 1, 0)\n",
    "df['outlier_bst_diff_traj'] = np.where(df.bst_diff_traj.values > 0.1, 1, 0)\n",
    "df['outlier_rf_diff_dur'] = np.where(df.rf_diff_dur.values > 0.1, 1, 0)\n",
    "df['outlier_rf_diff_traj'] = np.where(df.rf_diff_traj.values > 0.1, 1, 0)\n",
    "df['outlier_sum'] = df['outlier_bst_diff_dur']+\\\n",
    "df['outlier_bst_diff_traj'] + \\\n",
    "df['outlier_rf_diff_dur'] +\\\n",
    "df['outlier_rf_diff_traj']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([     0,      1,      2, ..., 465169, 465170, 465171])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_outlier_index = df[df.outlier_sum == 0].index.values\n",
    "len(non_outlier_index)\n",
    "non_outlier_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['non_outlier_index_stage1.pkl']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(non_outlier_index, 'non_outlier_index_stage1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rf_pred_traj_stage1.pkl']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(bst_pred_dur, 'bst_pred_dur_stage1.pkl')\n",
    "joblib.dump(bst_pred_traj, 'bst_pred_traj_stage1.pkl')\n",
    "joblib.dump(rf_pred_dur, 'rf_pred_dur_stage1.pkl')\n",
    "joblib.dump(rf_pred_traj, 'rf_pred_traj_stage1.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
