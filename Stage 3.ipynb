{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "Stage 3 is a little bit similar to Stage 2, as we are trying to fit an ensemble model using the training data to build prediction about the duration and trajlength from the test data. What is different is that We combine the information about the training data + trajlength to fit a model to predict duration, and then we fit model using training data + duration to predict trajlength. As we do not have any information about trajlength/duration in our test data, we will use our prediction from stage 2 as the values. Furthermore, I tried both using the predicted training duration and trajlength values from stage 2 to fit the model and the true training duration and trajlength values to fit the model. Unfortunately, both set of values produces similar result\n",
    "\n",
    "I believe that the information about trajlength is crucial in predicting duration and vice versa. This will help our model to learn about the duration covariates when predicting trajlength, and vice versa. This improves the RMSPE in our Kaggle Submission significantly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import train_test_split, \n",
    "GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor,\n",
    "GradientBoostingRegressor\n",
    "from sklearn.linear_model import LassoCV, ElasticNetCV\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "import xgboost as xgb\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import load_model\n",
    "from keras.layers import Input, Dense,\n",
    "BatchNormalization, Dropout, Activation\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from scipy.sparse import coo_matrix, hstack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to stage 2, we will load the train and test data after the feature engineering in Stage 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(465172, 2701)\n",
      "(465172, 2701)\n"
     ]
    }
   ],
   "source": [
    "sX_train_stage0 = joblib.load('sX_train_stage0.pkl')\n",
    "sX_test_stage0 = joblib.load('sX_test_stage0.pkl')\n",
    "print sX_train_stage0.shape\n",
    "print sX_test_stage0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also load the true values of the trajlength and duration from the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_price = joblib.load( 'Y_train_price.pkl')\n",
    "Y_train_duration = joblib.load('Y_train_duration.pkl')\n",
    "Y_train_trajlength = joblib.load('Y_train_trajlength.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other than that, we load the prediction for the training values for both duration and trajlength in Stage 2 using the Ensemble Model (Lasso Model fitted with the prediction from the Random Forest Model and XGBoost Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_dur_pred = \\\n",
    "joblib.load('Y_train_dur_pred_stage2v2.pkl')\n",
    "Y_train_traj_pred = \\\n",
    "joblib.load('Y_train_traj_pred_stage2v2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(465172, 2701)\n",
      "(465172, 2701)\n",
      "(465172,)\n",
      "(465172,)\n",
      "(465172,)\n"
     ]
    }
   ],
   "source": [
    "print sX_train_stage0.shape\n",
    "print sX_test_stage0.shape\n",
    "print Y_train_price.shape\n",
    "print Y_train_duration.shape\n",
    "print Y_train_trajlength.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we will remove all training values that are deemed as an outlier from Stage 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(464151,)\n",
      "[     0      1      2 ... 465169 465170 465171]\n"
     ]
    }
   ],
   "source": [
    "non_outlier_index_stage1 = \\\n",
    "np.array(joblib.load('non_outlier_index_stage1.pkl'))\n",
    "print non_outlier_index_stage1.shape\n",
    "print non_outlier_index_stage1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then create the new features to be fitted using the ensemble model in this stage. For Ensemble Model that predicts duration, we will use the original features generated from stage 0 + Predicted value of the trajlength from Stage 2. For Ensemble Model that predicts trajlength, we will use the original features generated from stage 0 + Predited value of the duration from Stage 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sX_train_stage0 = sX_train_stage0[non_outlier_index_stage1]\n",
    "\n",
    "sX_train_stage0_duration = \\\n",
    "hstack([sX_train_stage0, Y_train_dur_pred.reshape(-1, 1)])\n",
    "sX_train_stage0_trajlength = \\\n",
    "hstack([sX_train_stage0, Y_train_traj_pred.reshape(-1, 1)])\n",
    "\n",
    "sX_train_dur = sparse.csc_matrix(sX_train_stage0_duration)\n",
    "sX_train_traj = sparse.csc_matrix(sX_train_stage0_trajlength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_dur_stage2 = joblib.load('Y_dur_stage4.pkl')\n",
    "Y_traj_stage2 = joblib.load('Y_traj_stage4.pkl')\n",
    "Y_dur_stage2 = Y_dur_stage2.reshape(-1, 1)\n",
    "Y_traj_stage2 = Y_traj_stage2.reshape(-1, 1)\n",
    "\n",
    "sX_test_dur = hstack((sX_test_stage0, Y_dur_stage2))\n",
    "sX_test_traj = hstack((sX_test_stage0, Y_traj_stage2))\n",
    "sX_test_dur = sparse.csc_matrix(sX_test_dur)\n",
    "sX_test_traj = sparse.csc_matrix(sX_test_traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_pri = Y_train_price[non_outlier_index_stage1]\n",
    "Y_train_dur = Y_train_duration[non_outlier_index_stage1]\n",
    "Y_train_traj = Y_train_trajlength[non_outlier_index_stage1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = sX_train_dur.shape[0]\n",
    "n_test = sX_test_dur.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain_dur = xgb.DMatrix(sX_train_traj, label = Y_train_dur)\n",
    "dtrain_traj = xgb.DMatrix(sX_train_dur, label = Y_train_traj)\n",
    "dtest_dur = xgb.DMatrix(sX_test_traj)\n",
    "dtest_traj = xgb.DMatrix(sX_test_dur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(464151,)\n",
      "(464151,)\n",
      "(464151,)\n"
     ]
    }
   ],
   "source": [
    "print Y_train_pri.shape\n",
    "print Y_train_traj.shape\n",
    "print Y_train_dur.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(464151, 2702)\n",
      "(464151, 2702)\n",
      "(465172, 2702)\n",
      "(465172, 2702)\n"
     ]
    }
   ],
   "source": [
    "print sX_train_dur.shape\n",
    "print sX_train_traj.shape\n",
    "print sX_test_dur.shape\n",
    "print sX_test_traj.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to Stage 2, I actually perform train-validation split on the training data to choose appropriate parameters for my Ensemble Model. I will try the different parameters of the model by fitting it on the 80% of my training dataset, and check the performance on 20% of my training dataset by checking the RMSPE of the prediction vs the true values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_fit, idx_val = \\\n",
    "train_test_split(np.arange(n_train), test_size = 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sX_fit_dur = sX_train_dur[idx_fit]\n",
    "sX_fit_traj = sX_train_traj[idx_fit]\n",
    "\n",
    "sX_val_dur = sX_train_dur[idx_val]\n",
    "sX_val_traj = sX_train_traj[idx_val]\n",
    "\n",
    "Y_fit_pri = Y_train_pri[idx_fit]\n",
    "Y_fit_dur = Y_train_dur[idx_fit]\n",
    "Y_fit_traj = Y_train_traj[idx_fit]\n",
    "\n",
    "Y_val_pri = Y_train_pri[idx_val]\n",
    "Y_val_dur = Y_train_dur[idx_val]\n",
    "Y_val_traj = Y_train_traj[idx_val]\n",
    "\n",
    "dfit_dur = xgb.DMatrix(sX_fit_traj, label = Y_fit_dur)\n",
    "dfit_traj = xgb.DMatrix(sX_fit_dur, label = Y_fit_traj)\n",
    "dval_dur = xgb.DMatrix(sX_val_traj, label = Y_val_dur)\n",
    "dval_traj = xgb.DMatrix(sX_val_dur, label = Y_val_traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(371320, 2702)\n",
      "(371320, 2702)\n",
      "(92831, 2702)\n",
      "(92831, 2702)\n",
      "(371320,)\n",
      "(371320,)\n",
      "(371320,)\n",
      "(92831,)\n",
      "(92831,)\n",
      "(92831,)\n"
     ]
    }
   ],
   "source": [
    "print sX_fit_dur.shape\n",
    "print sX_fit_traj.shape\n",
    "print sX_val_dur.shape\n",
    "print sX_val_traj.shape\n",
    "print Y_fit_pri.shape\n",
    "print Y_fit_dur.shape\n",
    "print Y_fit_traj.shape\n",
    "print Y_val_pri.shape\n",
    "print Y_val_dur.shape\n",
    "print Y_val_traj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "def rmpse_loss_func(ground_truth, predictions):\n",
    "    err = \\\n",
    "    np.sqrt(np.mean\\\n",
    "            ((np.true_divide(predictions,\n",
    "                             ground_truth) - 1.)**2))\n",
    "    return err\n",
    "\n",
    "rmpse_loss  = make_scorer(rmpse_loss_func, \n",
    "                          greater_is_better=False)\n",
    "\n",
    "def rmpse(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    err = np.sqrt\\\n",
    "    (np.mean((np.true_divide(preds,\n",
    "                             labels) - 1.)**2))\n",
    "    return 'error', err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(n_splits = 3, shuffle = True, random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predict the duration values using ensemble model in Stage 3, I use ensemble model by fitting the prediction from\n",
    "\n",
    "- Random Forest\n",
    "- XGBoost\n",
    "- Elastic Net\n",
    "- Lasso\n",
    "\n",
    "as a covariate to the Ensemble Lasso Model. We will then run trhough the test dataset to each of the base model, and use the result as the covariate for the ensemble model to come out with the final prediction for the duration\n",
    "\n",
    "The covariate for each of the base model is the features that we have engineered in the training data in Stage 0 + Predicted Trajectory values in Stage 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, I perform Gridsearch on the XGBoost by varying the parameters of `max_depth`, `min_child_weight`, and `gamma`, and `colsample_bytree`. From this experiment, the two most important parameters to tune are `max_depth` and `gamma`. Without setting `gamma` to be a positive number, the trees can easily overfit as it deccreases loss error on training data but not validation dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#i = 0\n",
    "#bst_dur_dict = {}\n",
    "#for max_depth in [15, 20, 25, 30, 40]:\n",
    "#    for min_child_weight in [1, 3, 5]:\n",
    "#        for gamma in [0, 1]:\n",
    "#            for colsample_bytree in [0.7, 1.0]:\n",
    "#                print \"idx:\"+str(i)\n",
    "#                param = { 'objective' : \"reg:linear\", \n",
    "#                          'booster' : \"gbtree\",\n",
    "#                          'eta'                 :0.03, \n",
    "#                          'max_depth'           :max_depth, \n",
    "#                          'colsample_bytree'    :colsample_bytree,\n",
    "#                          'min_child_weight' : min_child_weight,\n",
    "#                          'gamma' : gamma,\n",
    "#                         'subsample' : 0.7,\n",
    "#                          'n_thread' : 8\n",
    "#                        }\n",
    "#                bst_dur = xgb.train(param, dfit_dur, \\\n",
    "#evals=[(dfit_dur, 'fit'), (dval_dur, 'val')], num_boost_round = 500,\n",
    "#feval= rmpse, maximize = False)\n",
    "#                rmpse_val = rmpse_loss_func(Y_val_dur , bst_dur.predict(dval_dur))\n",
    "#                bst_dur_dict[i] = {\n",
    "#                    'max_depth' : max_depth,\n",
    "#                    'min_child_weight' : min_child_weight,\n",
    "#                    'gamma' : gamma,\n",
    "#                    'colsample_bytree' : colsample_bytree,\n",
    "#                    'rmpse' : rmpse_val\n",
    "#                }\n",
    "#                i = i + 1\n",
    "#joblib.dump(bst_dur_dict, 'bst_dur_dict.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>colsample_bytree</th>\n",
       "      <td>0.700000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.70000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.70000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gamma</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_depth</th>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>40.00000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>40.00000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min_child_weight</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rmpse</th>\n",
       "      <td>0.029942</td>\n",
       "      <td>0.030059</td>\n",
       "      <td>0.031755</td>\n",
       "      <td>0.031766</td>\n",
       "      <td>0.029852</td>\n",
       "      <td>0.030116</td>\n",
       "      <td>0.031733</td>\n",
       "      <td>0.031773</td>\n",
       "      <td>0.029868</td>\n",
       "      <td>0.030062</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03108</td>\n",
       "      <td>0.030923</td>\n",
       "      <td>0.030383</td>\n",
       "      <td>0.030026</td>\n",
       "      <td>0.03107</td>\n",
       "      <td>0.030889</td>\n",
       "      <td>0.030055</td>\n",
       "      <td>0.029804</td>\n",
       "      <td>0.031078</td>\n",
       "      <td>0.030907</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         0          1          2          3          4   \\\n",
       "colsample_bytree   0.700000   1.000000   0.700000   1.000000   0.700000   \n",
       "gamma              0.000000   0.000000   1.000000   1.000000   0.000000   \n",
       "max_depth         15.000000  15.000000  15.000000  15.000000  15.000000   \n",
       "min_child_weight   1.000000   1.000000   1.000000   1.000000   3.000000   \n",
       "rmpse              0.029942   0.030059   0.031755   0.031766   0.029852   \n",
       "\n",
       "                         5          6          7          8          9   \\\n",
       "colsample_bytree   1.000000   0.700000   1.000000   0.700000   1.000000   \n",
       "gamma              0.000000   1.000000   1.000000   0.000000   0.000000   \n",
       "max_depth         15.000000  15.000000  15.000000  15.000000  15.000000   \n",
       "min_child_weight   3.000000   3.000000   3.000000   5.000000   5.000000   \n",
       "rmpse              0.030116   0.031733   0.031773   0.029868   0.030062   \n",
       "\n",
       "                    ...            50         51         52         53  \\\n",
       "colsample_bytree    ...       0.70000   1.000000   0.700000   1.000000   \n",
       "gamma               ...       1.00000   1.000000   0.000000   0.000000   \n",
       "max_depth           ...      40.00000  40.000000  40.000000  40.000000   \n",
       "min_child_weight    ...       1.00000   1.000000   3.000000   3.000000   \n",
       "rmpse               ...       0.03108   0.030923   0.030383   0.030026   \n",
       "\n",
       "                        54         55         56         57         58  \\\n",
       "colsample_bytree   0.70000   1.000000   0.700000   1.000000   0.700000   \n",
       "gamma              1.00000   1.000000   0.000000   0.000000   1.000000   \n",
       "max_depth         40.00000  40.000000  40.000000  40.000000  40.000000   \n",
       "min_child_weight   3.00000   3.000000   5.000000   5.000000   5.000000   \n",
       "rmpse              0.03107   0.030889   0.030055   0.029804   0.031078   \n",
       "\n",
       "                         59  \n",
       "colsample_bytree   1.000000  \n",
       "gamma              1.000000  \n",
       "max_depth         40.000000  \n",
       "min_child_weight   5.000000  \n",
       "rmpse              0.030907  \n",
       "\n",
       "[5 rows x 60 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bst_dur_dict = joblib.load('bst_dur_dict.pkl')\n",
    "pd.DataFrame(bst_dur_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the grid search, I have decided to use the following values for my XGBoost for duration prediction. I choose a deep trees, with high values of gamma and minimum child weight. I believe that this model is complex enough to learn about the duration of each taxi ride based on the available information, while it's able to not overfit the training data due to the regularization parameter that I have set, `min_child_weight` and `gamma`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = { 'objective' : \"reg:linear\", \n",
    "          'booster' : \"gbtree\",\n",
    "          'eta'                 :0.05, \n",
    "          'max_depth'           : 30, \n",
    "          'colsample_bytree'    : 0.7,\n",
    "          'min_child_weight' : 3,\n",
    "          'gamma' : 1,\n",
    "          'subsample' : 0.7,\n",
    "          'n_thread' : 8\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst_dur = xgb.train(param, dtrain_dur, evals=[(dtrain_dur, 'train')], \n",
    "                num_boost_round = 2000, feval= rmpse, maximize = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Random Forest Regressor, I found out that the most crucial parameters to tune is the `max_features`, as it affects the number of parameters considered at each node. As we perform one-hot encoding, our matrix is very sparse. therefore, if we use the default value such as `log2` or `sqrt`, it might randomly choose unimportant features (features where all the values is zero). Thus, our model will be very biased. The max depth introduced in this Stage is quite small as my computer could not accomodate deep trees. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_dur = RandomForestRegressor(max_depth = 12, \n",
    "                               max_features = 0.3, \n",
    "                               n_estimators= 1000, \n",
    "                               verbose = 10, n_jobs = -1, \n",
    "                               criterion='mse', \n",
    "                               oob_score = True)\\\n",
    ".fit(sX_train_traj, Y_train_dur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Lasso Model and Elastic Net, i will use the inbuilt function under `sklearn` package which perform 3-Fold Cross validation with 100 different values for alphas. We will fit the training data + predicted trajectory values from Stage 2 to fit the model which predict the duration of the taxi ride."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_dur = LassoCV(n_jobs = -1, verbose = 3).\\\n",
    "fit(sX_train_traj, Y_train_dur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enet_dur = ElasticNetCV(n_jobs = -1, verbose = 3)\\\n",
    ".fit(sX_train_traj, Y_train_dur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then build an ensemble models, with the feature dimension (n_train, 4). Each feature consists of the prediction from the 4 models above - Random Forest, XGBoost, Lasso, and Elastic Net. We will also use Lasso for ensembling this 4 predictions, as we assume that the relationship between this 4 covariates should be linear. We will perform 3-fold Cross Validation with 100 unique values of alpha, using `LassoCV`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ensemble_dur = np.zeros((n_train, 4))\n",
    "X_ensemble_dur[:,0] = bst_dur.predict(dtrain_dur)\n",
    "X_ensemble_dur[:,1] = rf_dur.predict(sX_train_traj)\n",
    "X_ensemble_dur[:,2] = lm_dur.predict(sX_train_traj)\n",
    "X_ensemble_dur[:,3] = enet_dur.predict(sX_train_traj)\n",
    "lasso_dur = LassoCV(n_jobs=-1, verbose=1)\\\n",
    ".fit(X_ensemble_dur, Y_train_dur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lasso_dur_stage3v3.pkl']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(bst_dur, 'bst_dur_stage3v3.pkl')\n",
    "joblib.dump(rf_dur, 'rf_dur_stage3v3.pkl')\n",
    "joblib.dump(lm_dur, 'lm_dur_stage3v3.pkl')\n",
    "joblib.dump(enet_dur, 'enet_dur_stage3v3.pkl')\n",
    "joblib.dump(lasso_dur, 'lasso_dur_stage3v3.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, I didn't really have time to perform the grid search for the parameter for predicting the trajectory length values. Therefore, I assume that the optimal parameter for both XGBoost and RF model for predicting trajectory is rather similar to the one for duration, and thus I use similar parameter to fit my model for predicting the trajectory length values.\n",
    "\n",
    "Similar to duration, in predicting the trajectory values, I will create an ensemble model (Lasso) by using the result from 4 base model as the covariates for the ensemble model\n",
    "\n",
    "- XGBoost\n",
    "- Random Forest\n",
    "- Elastic Net\n",
    "- Lasso\n",
    "\n",
    "The model used is completely similar to the one which predict the duration values\n",
    "\n",
    "For each of the Base model, we will fit the training data with features engineered from stage 0 (minus the outlier training values indicated in stage 1) and the prediction for duration values in Stage 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = { 'objective' : \"reg:linear\", \n",
    "          'booster' : \"gbtree\",\n",
    "          'eta'                 :0.05, \n",
    "          'max_depth'           : 30, \n",
    "          'colsample_bytree'    : 0.7,\n",
    "          'min_child_weight' : 3,\n",
    "          'gamma' : 1,\n",
    "          'subsample' : 0.7,\n",
    "          'n_thread' : 8\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst_traj = xgb.train(param, dtrain_traj, evals=[(dtrain_traj, 'train')], \n",
    "                num_boost_round = 2000, feval= rmpse, maximize = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_traj = RandomForestRegressor(max_depth = 12,\n",
    "                                max_features = 0.3,\n",
    "                                n_estimators=1000,\n",
    "                                verbose = 10,\n",
    "                                n_jobs = -1, \n",
    "                                criterion='mse', \n",
    "                                oob_score = True)\\\n",
    ".fit(sX_train_dur, Y_train_traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_traj = LassoCV(n_jobs = -1,\n",
    "                  verbose = 3)\\\n",
    ".fit(sX_train_dur, Y_train_traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enet_traj = ElasticNetCV(n_jobs = -1, \n",
    "                         verbose = 3)\\\n",
    ".fit(sX_train_dur, Y_train_traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ensemble_traj = np.zeros((n_train, 4))\n",
    "X_ensemble_traj[:,0] = bst_traj.predict(dtrain_traj)\n",
    "X_ensemble_traj[:,1] = rf_traj.predict(sX_train_dur)\n",
    "X_ensemble_traj[:,2] = lm_traj.predict(sX_train_dur)\n",
    "X_ensemble_traj[:,3] = enet_traj.predict(sX_train_dur)\n",
    "lasso_traj = LassoCV(n_jobs = -1, verbose =1 )\\\n",
    ".fit(X_ensemble_traj, Y_train_traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lasso_traj_stage3v3.pkl']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(bst_traj, 'bst_traj_stage3v3.pkl')\n",
    "joblib.dump(rf_traj, 'rf_traj_stage3v3.pkl')\n",
    "joblib.dump(lm_traj, 'lm_traj_stage3v3.pkl')\n",
    "joblib.dump(enet_traj, 'enet_traj_stage3v3.pkl')\n",
    "joblib.dump(lasso_traj, 'lasso_traj_stage3v3.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Y_train_traj_pred_stage3v2.pkl']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train_traj_pred = lasso_traj.predict(X_ensemble_traj)\n",
    "joblib.dump(Y_train_traj_pred, 'Y_train_traj_pred_stage3v2.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fitting all the models using our training data, namely:\n",
    "\n",
    "- XGBoost for Duration\n",
    "- Random Forest for Duration\n",
    "- Lasso for Duration\n",
    "- Elastic Net for Duration\n",
    "- Lasso as the Ensemble model for Duration\n",
    "\n",
    "\n",
    "- XGBoost for Trajlength\n",
    "- Random Forest for Trajlength\n",
    "- Lasso for TrajLength\n",
    "- Elastic Net for Trajlength\n",
    "- Lasso as the Ensemble model for Trajlength\n",
    "\n",
    "We can then perfrom prediction for Duration and Trajlength values using our test data. We will fit Test Data from Stage 0 + predicted trajlength values from Stage 2 for test data to each of our base model in Duration, and then use the result as the input for our Lasso Ensemble model for Duration. This will help us to attain Final predicted value for Duration. Using Test data from Stage 0 + predicted duration values from Stage 2 for test Data, we can perform the same steps for Trajlength model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_ens = np.zeros((n_test, 4))\n",
    "X_test_ens[:,0] = bst_dur.predict(dtest_dur)\n",
    "X_test_ens[:,1] = rf_dur.predict(sX_test_traj)\n",
    "X_test_ens[:,2] = lm_dur.predict(sX_test_traj)\n",
    "X_test_ens[:,3] = enet_dur.predict(sX_test_traj)\n",
    "Y_test_dur_pred = lasso_dur.predict(X_test_ens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_ens = np.zeros((n_test, 4))\n",
    "X_test_ens[:,0] = bst_traj.predict(dtest_traj)\n",
    "X_test_ens[:,1] = rf_traj.predict(sX_test_dur)\n",
    "X_test_ens[:,2] = lm_traj.predict(sX_test_dur)\n",
    "X_test_ens[:,3] = enet_traj.predict(sX_test_dur)\n",
    "Y_test_traj_pred = lasso_traj.predict(X_test_ens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Y_test_traj_pred_stage3v2.pkl']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(Y_test_dur_pred, 'Y_test_dur_pred_stage3v3.pkl')\n",
    "joblib.dump(Y_test_traj_pred, 'Y_test_traj_pred_stage3v3.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our prediction is in log, so we have to use the exponential of the predicted values. Finally, we can add the predicted value of duration and trajlength to get the predicted value for price. We can then submit our result to Kaggle!:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_pri_pred = np.exp(Y_test_dur_pred) + np.exp(Y_test_traj_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = pd.read_csv(\"test.csv\").ID.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'ID': test_id,\n",
    "       'PRICE': Y_test_pri_pred}\n",
    "submission_df = pd.DataFrame(data = data)\n",
    "submission_df.to_csv(\"stage_3_v3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>465173</td>\n",
       "      <td>301.033741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>465174</td>\n",
       "      <td>274.367411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>465175</td>\n",
       "      <td>453.520390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>465176</td>\n",
       "      <td>853.708869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>465177</td>\n",
       "      <td>432.465489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>465178</td>\n",
       "      <td>492.616076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>465179</td>\n",
       "      <td>274.921103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>465180</td>\n",
       "      <td>875.136083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>465181</td>\n",
       "      <td>314.856632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>465182</td>\n",
       "      <td>255.092013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>465183</td>\n",
       "      <td>656.215898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>465184</td>\n",
       "      <td>260.258196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>465185</td>\n",
       "      <td>719.648942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>465186</td>\n",
       "      <td>605.092879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>465187</td>\n",
       "      <td>329.782338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>465188</td>\n",
       "      <td>346.331230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>465189</td>\n",
       "      <td>313.978037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>465190</td>\n",
       "      <td>690.267975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>465191</td>\n",
       "      <td>313.209007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>465192</td>\n",
       "      <td>310.009128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>465193</td>\n",
       "      <td>645.484674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>465194</td>\n",
       "      <td>377.988940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>465195</td>\n",
       "      <td>284.569421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>465196</td>\n",
       "      <td>548.652385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>465197</td>\n",
       "      <td>597.958827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>465198</td>\n",
       "      <td>645.078075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>465199</td>\n",
       "      <td>630.231837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>465200</td>\n",
       "      <td>284.203207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>465201</td>\n",
       "      <td>290.258940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>465202</td>\n",
       "      <td>306.147776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465142</th>\n",
       "      <td>930315</td>\n",
       "      <td>436.182791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465143</th>\n",
       "      <td>930316</td>\n",
       "      <td>303.499447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465144</th>\n",
       "      <td>930317</td>\n",
       "      <td>282.745440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465145</th>\n",
       "      <td>930318</td>\n",
       "      <td>464.294057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465146</th>\n",
       "      <td>930319</td>\n",
       "      <td>467.747910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465147</th>\n",
       "      <td>930320</td>\n",
       "      <td>322.303948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465148</th>\n",
       "      <td>930321</td>\n",
       "      <td>584.706082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465149</th>\n",
       "      <td>930322</td>\n",
       "      <td>661.181094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465150</th>\n",
       "      <td>930323</td>\n",
       "      <td>465.198113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465151</th>\n",
       "      <td>930324</td>\n",
       "      <td>250.457294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465152</th>\n",
       "      <td>930325</td>\n",
       "      <td>330.606869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465153</th>\n",
       "      <td>930326</td>\n",
       "      <td>284.813694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465154</th>\n",
       "      <td>930327</td>\n",
       "      <td>762.968805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465155</th>\n",
       "      <td>930328</td>\n",
       "      <td>232.279744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465156</th>\n",
       "      <td>930329</td>\n",
       "      <td>235.968997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465157</th>\n",
       "      <td>930330</td>\n",
       "      <td>424.835019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465158</th>\n",
       "      <td>930331</td>\n",
       "      <td>475.684142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465159</th>\n",
       "      <td>930332</td>\n",
       "      <td>830.492181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465160</th>\n",
       "      <td>930333</td>\n",
       "      <td>281.327817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465161</th>\n",
       "      <td>930334</td>\n",
       "      <td>429.934817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465162</th>\n",
       "      <td>930335</td>\n",
       "      <td>616.133874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465163</th>\n",
       "      <td>930336</td>\n",
       "      <td>469.316786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465164</th>\n",
       "      <td>930337</td>\n",
       "      <td>315.727491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465165</th>\n",
       "      <td>930338</td>\n",
       "      <td>240.851262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465166</th>\n",
       "      <td>930339</td>\n",
       "      <td>561.283538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465167</th>\n",
       "      <td>930340</td>\n",
       "      <td>572.155125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465168</th>\n",
       "      <td>930341</td>\n",
       "      <td>319.177851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465169</th>\n",
       "      <td>930342</td>\n",
       "      <td>449.624034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465170</th>\n",
       "      <td>930343</td>\n",
       "      <td>223.195997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465171</th>\n",
       "      <td>930344</td>\n",
       "      <td>354.721901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>465172 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID       PRICE\n",
       "0       465173  301.033741\n",
       "1       465174  274.367411\n",
       "2       465175  453.520390\n",
       "3       465176  853.708869\n",
       "4       465177  432.465489\n",
       "5       465178  492.616076\n",
       "6       465179  274.921103\n",
       "7       465180  875.136083\n",
       "8       465181  314.856632\n",
       "9       465182  255.092013\n",
       "10      465183  656.215898\n",
       "11      465184  260.258196\n",
       "12      465185  719.648942\n",
       "13      465186  605.092879\n",
       "14      465187  329.782338\n",
       "15      465188  346.331230\n",
       "16      465189  313.978037\n",
       "17      465190  690.267975\n",
       "18      465191  313.209007\n",
       "19      465192  310.009128\n",
       "20      465193  645.484674\n",
       "21      465194  377.988940\n",
       "22      465195  284.569421\n",
       "23      465196  548.652385\n",
       "24      465197  597.958827\n",
       "25      465198  645.078075\n",
       "26      465199  630.231837\n",
       "27      465200  284.203207\n",
       "28      465201  290.258940\n",
       "29      465202  306.147776\n",
       "...        ...         ...\n",
       "465142  930315  436.182791\n",
       "465143  930316  303.499447\n",
       "465144  930317  282.745440\n",
       "465145  930318  464.294057\n",
       "465146  930319  467.747910\n",
       "465147  930320  322.303948\n",
       "465148  930321  584.706082\n",
       "465149  930322  661.181094\n",
       "465150  930323  465.198113\n",
       "465151  930324  250.457294\n",
       "465152  930325  330.606869\n",
       "465153  930326  284.813694\n",
       "465154  930327  762.968805\n",
       "465155  930328  232.279744\n",
       "465156  930329  235.968997\n",
       "465157  930330  424.835019\n",
       "465158  930331  475.684142\n",
       "465159  930332  830.492181\n",
       "465160  930333  281.327817\n",
       "465161  930334  429.934817\n",
       "465162  930335  616.133874\n",
       "465163  930336  469.316786\n",
       "465164  930337  315.727491\n",
       "465165  930338  240.851262\n",
       "465166  930339  561.283538\n",
       "465167  930340  572.155125\n",
       "465168  930341  319.177851\n",
       "465169  930342  449.624034\n",
       "465170  930343  223.195997\n",
       "465171  930344  354.721901\n",
       "\n",
       "[465172 rows x 2 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
