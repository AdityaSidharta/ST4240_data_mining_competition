{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2\n",
    "\n",
    "In stage 2, we will perform the prediction on duration and trajlength using the non-outlier training dataset, with the new features engineered in Stage 0 and index outlier removal in Stage 1. In this stage, We will build a simple ensemble model using Random Forest and XGBoost, and the ensemble uses the Lasso Model. The model fitted using the training dataset can then be used to predict both duration values and trajlength values for the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import train_test_split, \n",
    "GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor,\n",
    "GradientBoostingRegressor\n",
    "from sklearn.linear_model import LassoCV\n",
    "import xgboost as xgb\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first load the dataset that we have created from Stage 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_stage0 = joblib.load( 'X_train_stage0.pkl')\n",
    "X_test_stage0 = joblib.load( 'X_test_stage0.pkl')\n",
    "Y_train_price = joblib.load( 'Y_train_price.pkl')\n",
    "Y_train_duration = joblib.load('Y_train_duration.pkl')\n",
    "Y_train_trajlength = joblib.load('Y_train_trajlength.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(465172, 1561)\n",
      "(465172, 1561)\n",
      "(465172,)\n",
      "(465172,)\n",
      "(465172,)\n"
     ]
    }
   ],
   "source": [
    "print X_train_stage0.shape\n",
    "print X_test_stage0.shape\n",
    "print Y_train_price.shape\n",
    "print Y_train_duration.shape\n",
    "print Y_train_trajlength.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then use the non-outlier index we found in stage1 to remove all the outliers in the training data. As we can see from this, 1021 training data are removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(464151,)\n",
      "[     0      1      2 ... 465169 465170 465171]\n"
     ]
    }
   ],
   "source": [
    "non_outlier_index_stage1 = joblib.load('non_outlier_index_stage1.pkl')\n",
    "print non_outlier_index_stage1.shape\n",
    "print non_outlier_index_stage1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_stage1 = X_train_stage0[non_outlier_index_stage1]\n",
    "X_test_stage1 = X_test_stage0\n",
    "Y_train_price = Y_train_price[non_outlier_index_stage1]\n",
    "Y_train_duration  = Y_train_duration[non_outlier_index_stage1]\n",
    "Y_train_trajlength = Y_train_trajlength[non_outlier_index_stage1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = X_train_stage1.shape[0]\n",
    "n_test = X_test_stage1.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(464151, 1561)\n",
      "(465172, 1561)\n",
      "(464151,)\n",
      "(464151,)\n",
      "(464151,)\n"
     ]
    }
   ],
   "source": [
    "print X_train_stage1.shape\n",
    "print X_test_stage1.shape\n",
    "print Y_train_duration.shape\n",
    "print Y_train_trajlength.shape\n",
    "print Y_train_price.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to find the appropriate parameter, we will split the trainin data 80%-20%. We will try different values of parameters for the Random Forest and Extreme Gradient Boosting Model and fit it in the 80% of the training data, and check the RMSPE for the remaining 20% of the training data. To save some space within the report, I will only show the code of the validation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_train, idx_val = train_test_split\\\n",
    "(np.arange(n_train), test_size = 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test_stage1\n",
    "\n",
    "X_full_train = X_train_stage1\n",
    "Y_full_train_dur = Y_train_duration\n",
    "Y_full_train_traj = Y_train_trajlength\n",
    "\n",
    "sX_full_train = sparse.csc_matrix(X_full_train)\n",
    "sX_test = sparse.csc_matrix(X_test)\n",
    "\n",
    "dtest = xgb.DMatrix(sX_test)\n",
    "dtrain_full_dur = xgb.DMatrix(sX_full_train, \n",
    "                              label= Y_full_train_dur)\n",
    "dtrain_full_traj = xgb.DMatrix(sX_full_train,\n",
    "                               label= Y_full_train_traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val = \\\n",
    "X_train_stage1[idx_train], X_train_stage1[idx_val]\n",
    "Y_train_dur, Y_val_dur =\\\n",
    "Y_train_duration[idx_train], Y_train_duration[idx_val]\n",
    "Y_train_traj, Y_val_traj =\\\n",
    "Y_train_trajlength[idx_train], Y_train_trajlength[idx_val]\n",
    "Y_train_pri, Y_val_pri =\\\n",
    "Y_train_price[idx_train], Y_train_price[idx_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(464151, 1561)\n",
      "(464151,)\n",
      "(464151,)\n",
      "(371320, 1561)\n",
      "(92831, 1561)\n",
      "(371320,)\n",
      "(92831,)\n",
      "(371320,)\n",
      "(92831,)\n",
      "(465172, 1561)\n"
     ]
    }
   ],
   "source": [
    "print X_full_train.shape\n",
    "print Y_full_train_dur.shape\n",
    "print Y_full_train_traj.shape\n",
    "print X_train.shape\n",
    "print X_val.shape\n",
    "print Y_train_dur.shape\n",
    "print Y_val_dur.shape\n",
    "print Y_train_traj.shape\n",
    "print Y_val_traj.shape\n",
    "print X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we perform one hot encoding, our matrix is large and sparse. Turning it from numpy array to sparse matrix boosts the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sX_train = sparse.csc_matrix(X_train)\n",
    "sX_val = sparse.csc_matrix(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain_dur = xgb.DMatrix(sX_train, label= Y_train_dur)\n",
    "dval_dur = xgb.DMatrix(sX_val, label=Y_val_dur)\n",
    "dtrain_traj = xgb.DMatrix(sX_train, label= Y_train_traj)\n",
    "dval_traj = xgb.DMatrix(sX_val, label =Y_val_traj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar in stage 1, we will build a costum function so that we are able to track the RMSPE loss when we are fitting our Random forest and XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "def rmpse_loss_func(ground_truth, predictions):\n",
    "    err = np.sqrt\\\n",
    "    (np.mean((np.true_divide\\\n",
    "              (predictions, ground_truth) - 1.)**2))\n",
    "    return err\n",
    "\n",
    "rmpse_loss  = make_scorer(rmpse_loss_func, \n",
    "                          greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmpse(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    err = np.sqrt(np.mean((np.true_divide\\\n",
    "                           (preds, labels) - 1.)**2))\n",
    "    return 'error', err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "watchlist_dur = [(dval_dur, 'eval_dur'),\n",
    "                 (dtrain_dur, 'train_dur')]\n",
    "watchlist_traj = [(dval_traj, 'eval_traj'),\n",
    "                  (dtrain_traj, 'train_traj')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final parameter used in the Stage 2 for XGboost is depicted as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = { 'objective' : \"reg:linear\", \n",
    "          'booster' : \"gbtree\",\n",
    "          'eta'                 :0.05, \n",
    "          'max_depth'           :12, \n",
    "          'colsample_bytree'    : 0.7,\n",
    "          'subsample' : 0.7,\n",
    "          'gamma' : 1,\n",
    "          'n_thread' : 8\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst_dur = xgb.train(param, dtrain_full_dur, \n",
    "                    evals=[(dtrain_full_dur,\n",
    "                            'train')], \n",
    "                num_boost_round = 2000, \n",
    "                    feval= rmpse, maximize = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter for the Random Forest model is depicted as follows. In hindsight, I should have chosen 0.3 as the max features instead of `sqrt` as it chooses too little features in each note. Furthermore, even though in practice we do not limit the max depth of Random Forest, I need to do that to save time in the computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_dur = RandomForestRegressor(max_depth = 22, \n",
    "                               max_features = 'sqrt',\n",
    "                               n_estimators=2000, \n",
    "                               verbose = 10, n_jobs = -1,\n",
    "                               criterion='mse')\\\n",
    ".fit(sX_full_train, Y_full_train_dur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then use the prediction from the XGBoost and the random forest as the input for the Lasso Model. In this case, we will perform a Cross validated lasso model, where we try 100 different value of alphas and take the best alpha which have the lowest RMSPE loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ensemble_dur = np.zeros((n_train, 2))\n",
    "X_ensemble_dur[:,0] = bst_dur.predict(dtrain_full_dur)\n",
    "X_ensemble_dur[:,1] = rf_dur.predict(sX_full_train)\n",
    "lasso_dur = LassoCV().fit(X_ensemble_dur, Y_full_train_dur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lasso_dur_stage2.pkl']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(bst_dur, 'bst_dur_stage2.pkl')\n",
    "joblib.dump(rf_dur, 'rf_dur_stage2.pkl')\n",
    "joblib.dump(lasso_dur, 'lasso_dur_stage2.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Trajlength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters for predicting Trajlength is very similar. Compared to the model which predict duration, the only difference is that we fit the RF and XGB model against the trajlength training values instead of duration training values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = { 'objective' : \"reg:linear\", \n",
    "          'booster' : \"gbtree\",\n",
    "          'eta'                 :0.05, \n",
    "          'max_depth'           :12, \n",
    "          'colsample_bytree'    : 0.7,\n",
    "          'subsample' : 0.7,\n",
    "          'gamma' : 1,\n",
    "          'n_thread' : 8\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst_traj = xgb.train(param, dtrain_full_traj,\n",
    "                     evals=[(dtrain_full_traj, 'train')], \n",
    "                num_boost_round = 2000,\n",
    "                     feval= rmpse, maximize = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_traj = \\\n",
    "RandomForestRegressor(max_depth = 22,\n",
    "                      max_features = 'sqrt',\n",
    "                      n_estimators=2000, verbose = 3,\n",
    "                      n_jobs = -1, criterion='mse'\\\n",
    "                     ).fit(sX_full_train, Y_full_train_traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ensemble_traj = np.zeros((n_train, 2))\n",
    "X_ensemble_traj[:,0] = bst_traj.predict(dtrain_full_traj)\n",
    "X_ensemble_traj[:,1] = rf_traj.predict(sX_full_train)\n",
    "lasso_traj = LassoCV().fit(X_ensemble_traj, Y_full_train_traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lasso_traj_stage2.pkl']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(bst_traj, 'bst_traj_stage2.pkl')\n",
    "joblib.dump(rf_traj, 'rf_traj_stage2.pkl')\n",
    "joblib.dump(lasso_traj, 'lasso_traj_stage2.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, I will show you the hyperparameter tuning process to get the parameter values used in the RF and XGB model for both predicting duration and trajlength. Performing Gridsearch / RandomGridSearch + CV is not a good idea in this case because of covariates and observation dimension is extremely large. Therefore, I increase the complexity of the model by intuition - adjusting the hyperparameter slowly if there are a lot of bias / if it started to overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bst_dur = xgb.train(param, dtrain_dur, evals=[(dtrain_dur, 'train')], \n",
    "                #num_boost_round = 2000, feval= rmpse, maximize = False)\n",
    "#rf_dur = RandomForestRegressor(max_depth = 22, max_features = 'sqrt', n_estimators=2000, \n",
    "                                #verbose = 3, n_jobs = -1, criterion='mse').fit(sX_dur, Y_train_dur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_ens = np.zeros((372137, 2))\n",
    "# X_train_ens[:,0] = bst_dur.predict(dtrain_dur)\n",
    "# X_train_ens[:,1] = rf_dur.predict(sX_train)\n",
    "# X_val_ens = np.zeros((93035, 2))\n",
    "# X_val_ens[:,0] = bst_dur.predict(dval_dur)\n",
    "# X_val_ens[:,1] = rf_dur.predict(sX_val)\n",
    "# print rmpse_loss(lasso_dur, X_train_ens, Y_train_dur)\n",
    "# print rmpse_loss(lasso_dur, X_val_ens, Y_val_dur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bst_traj = xgb.train(param, dtrain_traj, evals=[(dtrain_traj, 'train')], \n",
    "#                num_boost_round = 2000, feval= rmpse, maximize = False)\n",
    "#rf_traj = RandomForestRegressor(max_depth = 22, max_features = 'sqrt', n_estimators=2000, \n",
    " #                               verbose = 3, n_jobs = -1, criterion='mse').fit(sX_train, Y_train_traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_ens = np.zeros((372137, 2))\n",
    "# X_train_ens[:,0] = bst_traj.predict(dtrain_traj)\n",
    "# X_train_ens[:,1] = rf_traj.predict(sX_train)\n",
    "# X_val_ens = np.zeros((93035, 2))\n",
    "# X_val_ens[:,0] = bst_traj.predict(dval_traj)\n",
    "# X_val_ens[:,1] = rf_traj.predict(sX_val)\n",
    "# print rmpse_loss(lasso_traj, X_train_ens, Y_train_traj)\n",
    "# print rmpse_loss(lasso_traj, X_val_ens, Y_val_traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_val_ens = np.zeros((93035, 2))\n",
    "# X_val_ens[:,0] = bst_dur.predict(dval_dur)\n",
    "# X_val_ens[:,1] = rf_dur.predict(sX_val)\n",
    "# Y_val_dur_pred = lasso_dur.predict(X_val_ens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_val_ens = np.zeros((93035, 2))\n",
    "# X_val_ens[:,0] = bst_traj.predict(dval_traj)\n",
    "# X_val_ens[:,1] = rf_traj.predict(sX_val)\n",
    "# Y_val_traj_pred = lasso_traj.predict(X_val_ens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_val_pri_pred = np.exp(Y_val_dur_pred) + np.exp(Y_val_traj_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_val_pri_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.exp(Y_val_pri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20196613102493846"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rmpse_loss_func(np.exp(Y_val_pri), Y_val_pri_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we can fit the test dataset to get our prediction in Stage 2 for both duration and trajlength, using the model that we have fit using the training data. We can then save this prediction for the third stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_ens = np.zeros((n_test, 2))\n",
    "X_test_ens[:,0] = bst_dur.predict(dtest)\n",
    "X_test_ens[:,1] = rf_dur.predict(sX_test)\n",
    "Y_test_dur_pred = lasso_dur.predict(X_test_ens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_ens = np.zeros((n_test, 2))\n",
    "X_test_ens[:,0] = bst_traj.predict(dtest)\n",
    "X_test_ens[:,1] = rf_traj.predict(sX_test)\n",
    "Y_test_traj_pred = lasso_traj.predict(X_test_ens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_pri_pred = np.exp(Y_test_dur_pred) \\\n",
    "+ np.exp(Y_test_traj_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = pd.read_csv(\"test.csv\").ID.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'ID': test_id,\n",
    "       'PRICE': Y_test_pri_pred}\n",
    "submission_df = pd.DataFrame(data = data)\n",
    "submission_df.to_csv(\"stage_2_v1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Y_traj_stage2.pkl']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(Y_test_dur_pred, 'Y_dur_stage2.pkl')\n",
    "joblib.dump(Y_test_traj_pred, 'Y_traj_stage2.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
